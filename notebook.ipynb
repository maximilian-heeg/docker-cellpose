{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "110c006d",
   "metadata": {},
   "source": [
    "# Perform a nuclear segmentation on the DAPI staining using cellpose 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40302b73-6293-48bf-80b8-051dc653ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio as io\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tifffile\n",
    "from tqdm.notebook import tqdm\n",
    "import pathlib\n",
    "from cellpose import models, core\n",
    "import json\n",
    "\n",
    "xenium_path = 'data'\n",
    "\n",
    "DEVELOPMENT = os.getenv('DEVELOPMENT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de80d81-0399-4b42-a934-8dbcb8ccd606",
   "metadata": {},
   "source": [
    "## Read in Xenium DAPI\n",
    "\n",
    "In this part we import the DAPI OME TIFF, create a max projection of the different layers and downsample the image slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352633b0-14cf-499c-98eb-d2f4511f59dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dapi_image(path: str, downscale_factor: int = 2) -> np.ndarray:\n",
    "    img_fpath = pathlib.Path(os.path.join(path, 'morphology_mip.ome.tif'))\n",
    "    tif = tifffile.TiffFile(img_fpath)\n",
    "    img = tif.asarray()\n",
    "    if DEVELOPMENT:\n",
    "        print(img.shape)\n",
    "        img = img[10000:12000,10000:12000]\n",
    "    return downscale_image(img, downscale_factor=downscale_factor)\n",
    "\n",
    "def downscale_image(img: np.ndarray, downscale_factor: int = 2) -> np.ndarray:\n",
    "    # Calculate the amount of padding needed for each axis\n",
    "    pad_height = (downscale_factor - img.shape[0] % downscale_factor) % downscale_factor\n",
    "    pad_width = (downscale_factor - img.shape[1] % downscale_factor) % downscale_factor\n",
    "\n",
    "    # Pad the array with zeros\n",
    "    img = np.pad(img, ((0, pad_height), (0, pad_width)), mode='constant')\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991e7810-e4c1-4944-87f5-a31c27c5fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxed_xenium = read_dapi_image(xenium_path, downscale_factor=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e1557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxed_xenium.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e90c72-a70c-496b-852b-5efe2738496f",
   "metadata": {},
   "source": [
    "## Run cellpose\n",
    "\n",
    "Here, we use the pretrained model to perform a nuclear segmentation with cellpose.\n",
    "\n",
    "    This part takes A LOT of memory. Make sure you have at least 200 free Gb for a 35000 x 35000 pixel area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21499e78-a758-4070-a701-8b52f6c8491c",
   "metadata": {},
   "source": [
    "I'm putting the model in the models folder with all the notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e10b0f-6a01-4401-a0e0-dcd1eb7076f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cellpose(img: np.ndarray, model_path: str) -> (np.ndarray, np.ndarray, np.ndarray):\n",
    "    use_GPU = core.use_gpu()\n",
    "    model = models.CellposeModel(gpu=use_GPU, pretrained_model= model_path  )\n",
    "    channels = [0,0]\n",
    "    masks, flows, styles = model.eval([img], channels=channels, diameter=model.diam_labels,flow_threshold=0, cellprob_threshold=0)\n",
    "    return (masks, flows, styles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568580f6-62a8-46fe-9e60-845b144bd6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks, flows, styles = run_cellpose(\n",
    "    maxed_xenium,\n",
    "    model_path = r'models/DAPI'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c23162-de9c-4ef8-bb61-b4b2780c672e",
   "metadata": {},
   "source": [
    "Plot and save segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb527e3-2f76-4433-9c24-cc7a8b4d9eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEVELOPMENT:\n",
    "    plt.imshow(masks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148fc126-21f8-4988-b348-e0dd0a64e87d",
   "metadata": {},
   "source": [
    "## Add the new segmentation to the transcripts.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3ca54e-f59a-47a8-b8a4-d7fcffa2b50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_transcripts = pd.read_csv(os.path.join(xenium_path, 'transcripts.csv.gz'))\n",
    "detected_transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7abb448-6a39-4309-85bc-19e674d08751",
   "metadata": {},
   "source": [
    "Get the pixel to um conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222f3cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixel_size(path: str) -> float:\n",
    "    file = open(os.path.join(path, \"experiment.xenium\"))\n",
    "    experiment = json.load(file)\n",
    "    pixel_size = experiment['pixel_size']\n",
    "    return pixel_size\n",
    "\n",
    "pixel_size = get_pixel_size(xenium_path)\n",
    "pixel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774c7713-8738-47ae-adb0-5617867fc645",
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_transcripts['x_location_pixels'] = detected_transcripts.x_location.values*(1/pixel_size)\n",
    "detected_transcripts['y_location_pixels'] = detected_transcripts.y_location.values*(1/pixel_size)\n",
    "\n",
    "if DEVELOPMENT:\n",
    "    detected_transcripts = detected_transcripts[\n",
    "        (detected_transcripts['x_location_pixels'] > 10000) &\n",
    "        (detected_transcripts['x_location_pixels'] < 12000) &\n",
    "        (detected_transcripts['y_location_pixels'] > 10000) &\n",
    "        (detected_transcripts['y_location_pixels'] < 12000)\n",
    "    ].copy()\n",
    "    detected_transcripts['x_location_pixels'] = detected_transcripts['x_location_pixels'] - 10000\n",
    "    detected_transcripts['y_location_pixels'] = detected_transcripts['y_location_pixels'] - 10000\n",
    "\n",
    "    detected_transcripts['x_location'] = detected_transcripts.x_location_pixels.values*(pixel_size)\n",
    "    detected_transcripts['y_location'] = detected_transcripts.y_location_pixels.values*(pixel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317b0596-94cb-4a01-b7b1-4c2b0c75b43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26e4317-0948-4c97-bffb-f8cdb98699ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_cells = masks[0][detected_transcripts.y_location_pixels.values.astype(int), detected_transcripts.x_location_pixels.values.astype(int)]\n",
    "detected_transcripts['cell_id'] = detected_cells\n",
    "detected_transcripts['overlaps_nucleus'] = (detected_cells > 0).astype(int)\n",
    "detected_transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e3a478",
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_transcripts.to_csv(\"transcripts_cellpose.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
